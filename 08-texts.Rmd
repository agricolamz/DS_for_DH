---
output: html_document
editor_options: 
  chunk_output_type: console
---
# Работа с текстами: `gutenbergr`, `tidytext`, `udpipe` {#tidytext}

```{r,message=FALSE}
library(tidyverse)
```

```{r, echo=FALSE}
theme_set(theme_bw())
```


## Пакет `gutenbergr`

Пакет `gutenbergr` является API для очень старого [проекта Guttenberg](http://www.gutenberg.org/).

```{r}
library(gutenbergr)
```

Все самое важное в этом пакете хранится в датасете `gutenberg_metadata`

```{r}
str(gutenberg_metadata)
```

Например, сейчас мы можем понять, сколько книг на разных языках можно скачать из проекта:

```{r}
gutenberg_metadata %>% 
  count(language, sort = TRUE)
```

Как видно, в основном это тексты на английском. Сколько авторов в датасете?

```{r}
gutenberg_metadata %>% 
  count(author, sort = TRUE)
```

Сколько произведений Джейн Остин (не перепутайте с другими Остин) есть в датасете?

```{r}
gutenberg_metadata %>% 
  filter(author == "Austen, Jane") %>% 
  distinct(gutenberg_id, title)
```

Давайте скачаем "Эмму":

```{r download_emma, cache=TRUE}
emma <- gutenberg_download(158)
emma
```

Можно скачивать сразу несколько книг. Давайте добавим еще "Леди Сьюзен":

```{r download_books, cache=TRUE}
books <- gutenberg_download(c(158, 946), meta_fields = "title")
books
books %>% 
  count(title)
```

```{block, type = "rmdtask"}
Сколько уникальных заголовков из базы данных содержит "Sherlock Holmes"?
```

```{r, include=FALSE}
gutenberg_metadata %>% 
  filter(str_detect(title, "Sherlock Holmes")) %>% 
  distinct(title)
# 9
```

## Библиотека `tidytext`

Сейчас скачанные книги записаны в таблицу, где одна строка это один абзац. Хочется мочь посчитать слова. Для этого книги нужно привести в tidy формат и для этого написан пакет `tidytext` (онлайн книга доступна [здесь](https://www.tidytextmining.com/)):

```{r}
library(tidytext)
books %>% 
  unnest_tokens(word, text)
```

Теперь можно посчитать самые частотные слова в обоих произведениях:

```{r}
books %>% 
  unnest_tokens(word, text) %>% 
  count(title, word, sort = TRUE)
```

Ну... Это было ожидаемо. Нужно убрать стопслова. Английские стопслова встроены в пакет (переменная `stop_words`):

```{r}
books %>% 
  unnest_tokens(word, text) %>% 
  count(title, word, sort = TRUE) %>% 
  anti_join(stop_words)
```

```{block, type = "rmdtask"}
Постройте следующий график, на котором представлены самые частотные 20 слов каждого из произведений.
```

```{r, echo = FALSE}
books %>% 
  unnest_tokens(word, text) %>% 
  count(title, word, sort = TRUE) %>% 
  anti_join(stop_words) %>% 
  group_by(title) %>% 
  slice(1:20) %>% 
  ggplot(aes(word, n))+
  geom_col()+
  coord_flip()+
  facet_wrap(~title, scale = "free")
```





```{r, eval = FALSE, include=FALSE}
library(tidyverse)
library(udpipe)
dl <- udpipe_download_model(language = "russian")
udmodel_ru <- udpipe_load_model(file = dl$file_model)

txt <- read_lines("/home/agricolamz/Desktop/kolobok")
txt <- str_c(txt, collapse = " ")

x <- udpipe_annotate(udmodel_ru, x = txt)
x <- as.data.frame(x)

sentiment <- read_csv("https://raw.githubusercontent.com/text-machine-lab/sentimental/master/sentimental/word_list/russian.csv")

x %>% 
  left_join(sentiment, by = c("lemma" = "word")) ->
  x

x %>% 
  select(score) %>% 
  as.data.frame()
  group_by(sentence_id) %>% 
  summarise(sentiment = sum(score, na.rm = TRUE)) 
  mutate(sentiment_type = ifelse(sentiment>= 0, "positive", "negative")) %>% 
  ggplot(aes(sentence_id, sentiment, fill = sentiment_type))+
  geom_col()
```

Функция `unnest_tokens()` позволяет работать не только со словами, но и, напрмиер, с биграммами:

```{r}
books %>% 
  unnest_tokens(word, text, token = "ngrams", n = 2)
```

Поиск самых частотных слов --- не едиснственная задача, которую можно решать при работе с текстом. Иногда имеет смысл узнать распределение слов в произведении. Давайте посмотрим как распределены в романе "Эмма" фамилии главных героев:

```{r}
books %>% 
  filter(title == "Emma") %>% 
  unnest_tokens(word, text) %>% 
  mutate(narrative_time = 1:n()) %>% 
  filter(str_detect(word, "knightley$|woodhouse$|churchill$|fairfax$")) %>%  
  ggplot()+
      geom_vline(aes(xintercept = narrative_time))+
  facet_wrap(~word, ncol = 1)
```


## Пакет `udpipe`

Пакет `udpipe` представляет лемматизацию, морфологический и синтаксический анализ разных языков. Туториал можно найти [здесь](https://bnosac.github.io/udpipe/docs/doc1.html), там же есть список доступных языков.

```{r}
library(udpipe)
```


Модели качаются очень долго.
```{r download_en_model, cache=TRUE}
enmodel <- udpipe_download_model(language = "english")
```

Теперь можно распарсить какое-нибудь предложение:
```{r}
udpipe("The want of Miss Taylor would be felt every hour of every day.", object = enmodel)
```

Скачаем русскую модель:
```{r  download_ru_model, cache=TRUE}
rumodel <- udpipe_download_model(language = "russian-syntagrus")
```

```{r}
udpipe("Жила-была на свете крыса в морском порту Вальпараисо, на складе мяса и маиса, какао и вина.", object = rumodel)
```

После того, как модель загружена можно уже к ней обращаться просто по имени:

```{r}
udpipe("Жила-была на свете крыса в морском порту Вальпараисо, на складе мяса и маиса, какао и вина.", object = "russian-syntagrus-ud-2.4-190531.udpipe")
```

