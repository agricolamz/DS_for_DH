[
["корреляция-и-регрессия.html", "16 Корреляция и регрессия 16.1 Дисперсия и стандартное отклонение 16.2 z-преобразование 16.3 Ковариация 16.4 Корреляция 16.5 Регрессионный анализ", " 16 Корреляция и регрессия 16.1 Дисперсия и стандартное отклонение Дисперсия — мера разброса значений наблюдений относительно среднего. \\[\\sigma^2_X = \\frac{\\sum_{i = 1}^n(x_i - \\bar{x})^2}{n - 1},\\] где \\(x_1, ..., x_n\\) — наблюдения; \\(\\bar{x}\\) — среднее всех наблюдений; \\(X\\) — вектор всех наблюдений; \\(n\\) — количество наблюдений. Представим, что у нас есть следующие данные: Тогда дисперсия — это сумма квадратов расстояний от каждой точки до среднего выборки (пунктирная линия) разделенное на количество наблюдений - 1 (по духу эта мера — обычное среднее, но если вас инетересует разница смещенной и несмещенной оценки дисперсии, см. видео). Для того чтобы было понятнее, что такое дисперсия, давайте рассмотрим несколько расспределений с одним и тем же средним, но разными дисперсиями: В R дисперсию можно посчитать при помощи функции var(). set.seed(42) x &lt;- rnorm(20, mean = 50, sd = 10) var(x) ## [1] 172.2993 Проверим, что функция выдает то же, что мы записали в формуле. var(x) == sum((x - mean(x))^2)/(length(x)-1) ## [1] TRUE Так как дисперсия является квадратом отклонения, то часто вместо нее используют стандартное отклонение \\(\\sigma\\) — корень из дисперсии. В R ее можно посчитать при помощи функции sd(): sd(x) ## [1] 13.12628 sd(x) == sqrt(var(x)) ## [1] TRUE 16.2 z-преобразование 16.3 Ковариация Ковариация — эта мера ассоциации двух переменных. \\[cov(X, Y) = \\frac{\\sum_{i = 1}^n(x_i - \\bar{x})(y_i-\\bar{y})}{n - 1},\\] где \\((x_1, y_1), ..., (x_n, y_n)\\) — пары наблюдений; \\(\\bar{x}, \\bar{y}\\) — средние наблюдений; \\(X, Y\\) — векторы всех наблюдений; \\(n\\) — количество наблюдений. Представим, что у нас есть следующие данные: Тогда, согласно формуле, для каждой точки вычисляется следующая площадь (пуктирными линиями обозначены средние): Если значения \\(x_i\\) и \\(y_i\\) какой-то точки либо оба больше, либо оба меньше средних \\(\\bar{x}\\) и \\(\\bar{y}\\), то получившееся произведение будет иметь знак +, если же наоборот — знак -. На графике это показано цветом. Таким образом, если много красных прямоугольников, то значение суммы будет положительное и обозначать положительную связь (чем больше \\(x\\), тем больше \\(y\\)), а если будет много синий прямоугольников, то значение суммы отрицательное и обозначать положительную связь (чем больше \\(x\\), тем меньше \\(y\\)). Непосредственно значение ковариации не очень информативно, так как может достаточно сильно варьироваться от датасета к датасету. В R ковариацию можно посчитать при помощи функции cov(). set.seed(42) x &lt;- rnorm(10, mean = 50, sd = 10) y &lt;- x + rnorm(10, sd = 10) cov(x, y) ## [1] 18.72204 cov(x, -y*2) ## [1] -37.44407 Как видно, простое умножение на два удвоило значение ковариации, что показывает, что непосредственно ковариацию использовать для сравнения разных датасетов не стоит. Проверим, что функция выдает то же, что мы записали в формуле. cov(x, y) == sum((x-mean(x))*(y - mean(y)))/(length(x)-1) ## [1] TRUE 16.4 Корреляция 16.4.1 Корреляция Пирсона \\[\\rho_{X,Y} = \\frac{cov(X, Y)}{\\sigma_X\\times\\sigma_Y} = \\frac{1}{n-1}\\times\\sum_{i = 1}^n\\left(\\frac{x_i-\\bar{x}}{\\sigma_X}\\times\\frac{y_i-\\bar{y}}{\\sigma_Y}\\right),\\] где \\((x_1, y_1), ..., (x_n, y_n)\\) — пары наблюдений; \\(\\bar{x}, \\bar{y}\\) — средние наблюдений; \\(X, Y\\) — векторы всех наблюдений; \\(n\\) — количество наблюдений. Последнее уравнение показывает, что коэффициент корреляции Пирсона можно представить как среднее (с поправкой) произведение \\(z\\)-нормализованных значений двух переменных. Эта нормализация приводит к тому, что значения корреляции имеют те же свойства знака коэффициента что и ковариация: если коэффициент положительный (т. е. много красных прямоугольников) — связь между переменными положительная (чем больше \\(x\\), тем больше \\(y\\)), если коэффициент отрицательный (т. е. много синих прямоугольников) — связь между переменными отрицательная (чем больше \\(x\\), тем меньше \\(y\\)); значение корреляции имееет не зависимое от типа данных интеретация: если модуль коэффициента близок к 1 или ему равен — связь между переменными сильная, если модуль коэффициента близок к 0 или ему равен — связь между переменными слабая. Для того чтобы было понятнее, что такое корреляция, давайте рассмотрим несколько расспределений с разными значениями корреляции: Как видно из этого графика, чем ближе модуль корреляции к 1, тем боллее компактно расположены точки, чем ближе к 0, тем более рассеяны значения. Достаточно легко научиться приблизительно оценивать коэфициент корреляции на глаз, поиграв 2–5 минут в игру Guess the correlation. В R коэффициент корреляции Пирсона можно посчитать при помощи функции cor(). set.seed(42) x &lt;- rnorm(15, mean = 50, sd = 10) y &lt;- x + rnorm(15, sd = 10) cor(x, y) ## [1] 0.6659041 Проверим, что функция выдает то же, что мы записали в формуле. cor(x, y) == cov(x, y)/(sd(x)*sd(y)) ## [1] TRUE cor(x, y) == sum(scale(x)*scale(y))/(length(x)-1) ## [1] TRUE 16.4.2 Ранговые корреляции Спирмана и Кендала 16.5 Регрессионный анализ https://antoinesoetewey.shinyapps.io/statistics-202/ "]
]
